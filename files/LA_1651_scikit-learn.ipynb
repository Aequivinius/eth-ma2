{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a467093",
   "metadata": {},
   "source": [
    "# LA_1651\n",
    "\n",
    "Nachdem Sie in der `LA_1650` gesehen haben, wie eine ML-*pipeline* im einfachsten Fall aussehen kann, gehen wir nun in diesem gr√∂√üeren Auftrag einen Schritt weiter, und versuchen, einen weniger vorgetretenen Pfad zu begehen: Statt des vorbereiteten Iris-Datensatzes versuchen wir nun, ein Modell f√ºr unseren Titanic-Datensatz zu erstellen, und k√∂nnen somit auf einige Feinheiten eingehen!\n",
    "\n",
    "‚ö†Ô∏è In diesem Auftrag m√ºssen Sie viel [recherchieren](https://scikit-learn.org/stable/user_guide.html) und in den vorhergegangenen Auftr√§gen nachlesen, welche Funktionen wie heissen und welche Parameter diese haben - lassen Sie sich nicht frustrieren, *that's part of the job!* und mit etwas √úbung wird es Ihnen in Zukunft leichter fallen, schnell an die wichtigen Informationen zu kommen. Ansonsten l√§uft dieser Auftrag aber wie in den bisherigen Notizb√ºchern ab - f√ºllen Sie die Teile ein, die mit `# üëæ TODO` markiert sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef47f3e",
   "metadata": {},
   "source": [
    "## Teil 1: Laden der Daten\n",
    "\n",
    "Im `LA_1650` konnten wir den Iris-Datensatz direkt durch `sklearn` importieren; dieser [Spielzeug-Datensatz](https://scikit-learn.org/stable/datasets/toy_dataset.html) wird mitgeliefert. Aber nat√ºrlich k√∂nnen Sie nicht Ihre ML-Karriere nur auf mitgelieferten Datens√§tzen aufbauen, darum geht es in diesem ersten Schritt darum, wie Sie andere Datens√§tze f√ºr `sklearn` zug√§nglich machen. Gl√ºcklicherweise ist `sklearn` weitgehend mit `pandas` kompatibel, und so k√∂nnen Sie den Titanic-Datensatz laden, wie Sie es sich gewohnt sind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: Load the titanic_train data set into a pandas DataFrame named titanics\n",
    "# titanics.head() should then show you the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80017c6",
   "metadata": {},
   "source": [
    "## Teil 2: Bestimmen von *features* und *targets*\n",
    "\n",
    "Es ist die Frage ein wenig Ihnen √ºberlassen: Ausgelegt ist der Datensatz nat√ºrlich, vorherzusagen, ob jemand √ºberlebt hat oder nicht, ob von welchen Faktoren diese am st√§rksten abh√§ngt. Aber Sie k√∂nnten nat√ºrlich auch andere Dinge versuchen vorherzusagen (Ticketpreis abh√§ngig von Alter?). In jedem Fall m√ºssen Sie aber die Daten nun aufteilen, sodass Sie ein `DataFrame` namens `X` haben, in welchem sich die *features* befinden; und ein `y` mit den *targets*, die Sie vorhersagen m√∂chten. Suchen Sie also interessante *features* (in diesem Fall: `pclass`, `age`, `sex`, `fare`, `embarked`, `party_size`), auf denen Ihre Analyse aufbaut, und teilen Sie die urspr√ºngliche `DataFrame` anhand der ausgesuchten Spalten auf.\n",
    "\n",
    "Hier ist auch ein guter Punkt, an dem Sie *feature engineering* betreiben, also das Erstellen neuer Spalten, welche Informationen auf eine besonders n√ºtzliche Art darstellen. Sie k√∂nnen hier gut das Beispiel mit der `PartySize` aus der `LA_1618` √ºbernehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e97692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: Split the titanics data into suitable X and y variables\n",
    "# X.head() should now show a smaller table with the aforementioned features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f208b0a",
   "metadata": {},
   "source": [
    "## Teil 3: *preprocessing*\n",
    "\n",
    "Gehen wir davon aus, dass Sie `age`, `sex` und `fare` gew√§hlt haben. Dies Werte dieser Spalten k√∂nnen Sie nicht einfach *tel quel* verwenden, sondern diese Bed√ºrfen des *preprocessing*. Zun√§chst m√ºssen Sie sich √ºberlegen, wie Sie mit fehlenden Werten umgehen - was soll der Algorithmus tun, wenn er eine Zahl erwartet, und stattdessen ein `NaN` liest? Sie k√∂nnten einfach alle Zeilen mittels `.dropna()` l√∂schen, welche irgendwo `NaN` beinhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9cda3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code shows you how many rows contain NaN values\n",
    "# if your solutions of part 1 and 2 are correct, it\n",
    "# should show you 176\n",
    "len(X[X.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44e333",
   "metadata": {},
   "source": [
    "Das sind fast 20% unserer Daten, die wir l√∂schen w√ºrden - das ist zuviel! Besser also, wir √ºberlegen uns eine schlauere L√∂sung: Lesen Sie in der [Dokumentation unter 6.4.2](https://scikit-learn.org/stable/modules/impute.html#univariate-feature-imputation) nach, wie Sie `NaN`-Werte durch den Modus ersetzen, und f√ºllen Sie so fehlende Werte auf! Beachten Sie dabei:\n",
    "\n",
    "* Sie m√ºssen `numpy` importieren, damit Sie `np.nan` verwenden k√∂nnen\n",
    "* Die `transform`-Funkion gibt Ihnen statt eine `DataFrame` ein einfaches Array zur√ºck, welches Sie wieder zur√ºck-konvertieren m√ºssen: Verwenden Sie `pd.DataFrame(`*`einfaches_array`*`columns=X.columns)` hierf√ºr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: Fill NaNs with modes as shown in the documentation\n",
    "# if all goes well, the code below should display 0 now\n",
    "len(X[X.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d026e",
   "metadata": {},
   "source": [
    "Gehen wir einen Schritt weiter - ML-Algorithmen sch√§tzen als `string` dargestellte kategorische Daten nicht (sehen Sie ggf. in der `PR_1604_Datentypen` nach), weshalb wir diese ersetzen m√ºssen. Statt `male` soll also jeweils `0`, und statt `female` jeweils `1` stehen - aber wir wollen das nat√ºrlich nicht von Hand umsetzen, sondern verwenden einen `OrdinalEncoder`, welcher das f√ºr uns √ºbernimmt: Lesen Sie hierf√ºr zun√§chst das [Kapitel 6.3.4](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features). Den dort auch erw√§hnten `OneHotEncoder` k√∂nnen Sie gleich verwenden, um die `embarked`-Spalte zu enkodieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d34b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: Use OrdinalEncoder for the sex-column, \n",
    "# and the OneHotEncoder for the embarked-column\n",
    "# Make sure you don't apply one of them to the whole\n",
    "# dataframe, but on individual columns only!\n",
    "# so use fit(X[[ relevant_column ]]) and \n",
    "# transform(X[[ relevant_column ]]), saving the result\n",
    "# again in X[[ relevant_column ]]\n",
    "\n",
    "# For the OneHotEncoder, follow the same procedure \n",
    "# as for the OrdinalEncoder. However, because the OHE\n",
    "# returns you multiple columns (one for London as your\n",
    "# port of embarkment, one for Queenstown and one for\n",
    "# Southhampton), you need to work a little bit to fit\n",
    "# all together again in the end: with 'embarks' being\n",
    "# the result of ohe.transform(), do the following:\n",
    "# this will glue the newly created columns to our\n",
    "# existing X-dataframe and delete the old 'embarked'\n",
    "# column. X.head should now show 8 columns.\n",
    "embarks = pd.DataFrame(embarks.toarray(), columns=[\"London\", \"Queenstown\", \"Southampton\"])\n",
    "X = pd.concat([X, embarks], axis=1)\n",
    "X.drop([\"embarked\"],  axis=1, inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20822892",
   "metadata": {},
   "source": [
    "Zum Abschluss dieses Teils, und zur Repetition, werden die `age`- und `fare`-Werte normalisiert - Sie k√∂nnen hierzu entweder den `MinMaxScaler` verwenden, den Sie aus der `LA_1650` kennen, oder sich am [`StandardScaler`](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) versuchen. Letzter grenzt die Werte nicht auf zwischen 0 und 1 ein, sondern berechnet die Werte so, dass deren Mittel bei 0 liegt und ihre Varianz bei 1.\n",
    "\n",
    "Zun√§chst muss unser `standard_scaler` allerdings berechnen, was das Mittel und die Varianz sind. Dann k√∂nnen wir ihn verwenden, um sowohl unsere `X`-Daten wie auch am Ende neue Daten, f√ºr die wir eine Vorhersage treffen m√∂chten, konsistenz zu skalieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adefbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = preprocessing.StandardScaler().fit(X[[\"age\", \"fare\"]])\n",
    "\n",
    "def scale(dataframe, columns, scaler):\n",
    "    scaled = scaler.transform(dataframe[columns])\n",
    "    dataframe[columns] = pd.DataFrame(scaled, columns=columns)\n",
    "    return dataframe\n",
    "\n",
    "# üëæ TODO: use the above scale-function on X for the columns\n",
    "# in question. X.head() should show the same result as above,\n",
    "# but with the age and fare values being much smaller now.\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a542116",
   "metadata": {},
   "source": [
    "## Teil 4: *test-train-split*\n",
    "\n",
    "Dieser Teil wird Ihnen dank `pandas` sehr einfach gemacht. Teilen Sie Ihre `X` und `y` in `train_X`, `train_y`, `test_X` und `test_y` ein, und zwar so, dass 20% der Daten zum Testen verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: Split the titanic data into suitable test and train sets\n",
    "# the code below should show (680, 170)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05f608",
   "metadata": {},
   "source": [
    "## Teil 5: Algorithmuswahl\n",
    "\n",
    "*Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.*, wobei mit *estimator* hier der Algorithmus gemeint ist. Es gibt nicht eine zwingende Zuordnung √† la *der Algorithmus XYZ ist immer der Beste*, sondern die Wahl h√§ngt von Ihrem Problem, Ihren Daten, Ihren verf√ºgbaren Ressourcen und Ihrer Erfahrung im *parameter tuning* ab. Wir greifen hier bereits etwas vor und zeigen Ihnen eine beliebte Technik, um einen passenden Algorithmus zu finden; und gehen im Anschlu√ü auf diesen Auftrag auf einige verwandte Themen vertieft ein.\n",
    "\n",
    "Diese Technik, die auch bei Ge√ºbteren oft zum Einsatz kommt, ist es, eine Batterie von Algorithmen als Kandidaten auszuw√§hlen, alle diese auf den *train*-Daten laufenzulassen und daraufhin zu √ºberpr√ºfen, welcher dieser Kandidaten auf den Test-Daten die genauesten Vorhersagen trifft.\n",
    "\n",
    "Im nachfolgenden Block wurden schon einige passende Algorithmen zusammengestellt, welche Sie nun nach der Reihe laufen lassen k√∂nnen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c237899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "algorithms = {\n",
    "    \"Nearest Neighbors\" : KNeighborsClassifier(3),\n",
    "    \"Stochastic Gradient Descent\" : SGDClassifier(),\n",
    "    \"Linear SVM\" : SVC(kernel=\"linear\", C=0.025),\n",
    "    \"Gaussian Process\" : GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    \"Decision Tree\" : DecisionTreeClassifier(max_depth=5),\n",
    "    \"Random Forest\" : RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"Neural Net\" : MLPClassifier(alpha=1, max_iter=1000),\n",
    "    \"Naive Bayes\" : GaussianNB(),\n",
    "    \"LDA\" : LinearDiscriminantAnalysis(),\n",
    "}\n",
    "\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    # üëæ TODO: Run each algorithm on X_train and y_train\n",
    "    # using the .fit function; then evaluate it using\n",
    "    # algorithm.score(X_test, y_test) and print the\n",
    "    # name and score for each algorithm - which one \n",
    "    # performs the best?\n",
    "    \n",
    "    # You might need to use y_train.values.ravel()\n",
    "    # rather than y_train in the fit()-function\n",
    "    \n",
    "    # You should get a list of the names of the algorithm\n",
    "    # and their score, something between 0.0 and 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510e63a",
   "metadata": {},
   "source": [
    "Wenn Sie den obigen Code-Block mehrfach ausf√ºhren, stellen Sie m√∂glicherweise kleine Variationen fest: Das liegt daran, dass nicht alle Algorithmen deterministisch ablaufen, sondern zu einem kleinen Grad zuf√§llig. Trotzdem sollte sich unser bereits bekannter Freund *Entscheidungsbaum* (und sein Artverwandter, der *Random Forest*) ziemlich weit oben in der Rangreihenfolge befinden. \n",
    "\n",
    "Was diese anderen Modelle bedeuten; was die Parameter wie Beispielsweise `MLPClassifier(alpha=1, max_iter=1000)` bedeuten und wie der `score` berechnet wird, darauf wird in sp√§teren Auftr√§gen genauer eingegangen. Doch f√ºr's erste haben Sie den schwierigsten Teil geschafft! üëè \n",
    "\n",
    "## Teil 6: Modell speichern\n",
    "\n",
    "Sehen Sie in der `LA_1650` und der [Dokumentation](https://scikit-learn.org/stable/modules/model_persistence.html) nach, wie Sie das beste Modell speichern und wieder laden, damit Sie es in Zukunft bereit haben, sollte wieder ein Atlantikdampfer einen Eisberg √ºbersehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: Save the best model with titanic_algorithm.joblib as\n",
    "# its filename, then load it again into the 'best_model' variable. \n",
    "# Note: The trained model is saved in the appropriate entry in \n",
    "# the algorithms-dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88a96d",
   "metadata": {},
   "source": [
    "## Teil 7: Vorhersagen treffen\n",
    "\n",
    "Sie kennen vermutlich den 1997 Film √ºber die Titanic - wenn nicht, planen Sie sich mittelfristig 2.5 Stunden Zeit ein f√ºr ein St√ºck Hollywood'scher Zeitgeschichte. Auf jeden Fall: In diesem Film besteigen Leonardo di Caprio und Kate Winslet die Titanic, verlieben sich und, nun ja, bleiben vom Untergang der Titanic nicht verschont.\n",
    "\n",
    "Fragen Sie Ihr Modell, wie warscheinlich es f√ºr die beiden jeweils ist, dass sie den Untergang √ºberleben. Leonardo spielt einen 20-j√§hrigen, der bei einem Pokerspiel eine Drittklass-Fahrkarte gewinnt (nehmen wir an mit einem Wert von 7.5 Pfund, was heute immerhin 850 CHF entspr√§che); Kate eine 17-j√§hrige, welche erste Klasse (f√ºr angenommene, l√§ppische 99.95 Pfund) f√§hrt; im Schlepptau Ihre Mutter und Ihren Mann, den Sie mit Leonardo betr√ºgt. Beide steigen in Southampton ein.\n",
    "\n",
    "‚ö†Ô∏è Denken Sie daran, dass Sie die gleichen Transformationen auf Ihre Daten anwenden m√ºssen, die Sie f√ºr die *train*-Daten angewendet haben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: Create a new dataframe called 'leo' or 'kate' with the appropriate values\n",
    "# Remember that our X-data has the following order:\n",
    "# pclass \tage \tsex \tfare \tparty_size \tLondon \tQueenstown \tSouthampton\n",
    "# Then, use the standard_scaler and the scale()-function\n",
    "# defined above to scale your dataframe. The code below should\n",
    "# show you a table with just one row, with fare and age values scaled appropriatedly\n",
    "leo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëæ TODO: finally, use the predict()-function\n",
    "# to see what would've happened to them. The return\n",
    "# value of the predict()-function is an array, of\n",
    "# which array[0] contains the classification:\n",
    "# 1 for survival, 0 for, well, not surviving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d44248",
   "metadata": {},
   "source": [
    "## Bonus-Etappe\n",
    "\n",
    "Sie bekommen beim Herumspielen sicher ein Gef√ºhl daf√ºr, dass gewisse Merkmale eine gr√∂√üe Auswirkung auf das Resultat haben als andere. Eine M√∂glichkeit, wissenschaftlich an die Frage heranzugehen, wie gro√ü diese Auswirkung ist, ist die Folgende: Das Modell wird neu berechnet und gleich auf den Test-Daten gepr√ºft, aber bei jedem Durchgang wird ein Merkmal (also einmal das Alter, einmal das Geschlecht etc.) weggelassen. Nun l√§sst sich herausfinden, wie die Leistung des Modells sinkt: Wird ein wichtiges Merkmal weggelassen, dann sinkt die Leistung betr√§chtlich; sinkt sie nur wenig, war das weggelassene Merkmal nicht wichtig. Diese Technik nennt sich *permutation importance*. Zum Gl√ºck m√ºssen Sie das nicht von Hand tun, sondern k√∂nnen auf praktische Funktionen zur√ºckgreifen!\n",
    "\n",
    "‚ö†Ô∏è Wenn gewisse Merkmale *abh√§ngig* voneinander sind, das heisst, wenn sich das eine Merkmal gut durch das andere vorhersagen l√§sst, dann funktioniert die *permutation importance* weniger gut. In unserem Beispiel ist das Resultat also mit vorsicht zu genie√üen, weil vermutlich `pclass` und `fare` von einander abh√§ngen - eine Fahrkarte erster Klasse ist teurer als eine dritter Klasse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "feature_names = \n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=X_train.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcb2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
