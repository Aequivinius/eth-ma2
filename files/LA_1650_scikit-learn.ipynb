{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889a9ae6",
   "metadata": {},
   "source": [
    "# LA_1650\n",
    "\n",
    "Dieses Notizbuch ist als Begleitung zur `PR_1650` gedacht, und soll den groben Ablauf der einzelnen Schritte einer ML-*pipeline* demonstrieren. Sie m√ºssen die einzelnen Schritte nicht abschlie√üend begreifen, aber eine Ahnung davon haben, was etwa im jeweiligen Schritt passiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d453ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06776d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# the as_frame parameter ensures we're\n",
    "# getting the data as pandas DataFrame\n",
    "iris = load_iris(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can't do this for every data set\n",
    "# sometimes you have to define yourself\n",
    "# what the features are and what their \n",
    "# names could be. But in this case, we're\n",
    "# lucky, since this was already done for us\n",
    "# Check the documentation for more information\n",
    "# about the dataset and try out some of the \n",
    "# commands listed here:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this, too, you'd normally have to define yourself\n",
    "# but again, it's already done for us.\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# so our X data has sepal length,\n",
    "# sepal width, petal length, petal width\n",
    "# and our y data is 0 for setosa, \n",
    "# 1 for versicolor and 2 for virginica\n",
    "X[:10], y[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead914ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block a few times to see how the random_state\n",
    "# parameter changes the way the data is split (with X_train[:10])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size = 0.3, random_state = 42\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc67618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we saw above, the sepal length is always much\n",
    "# larger than the petal width - so let's normalise\n",
    "# our data. In this step we'd also do other conversions\n",
    "# such as mapping the mapping our target data to \n",
    "# numbers instead of using string, but as we saw\n",
    "# above, this was already done for us, too.\n",
    "\n",
    "# check out the docs to see all the preprocessing possibilities: \n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "# note that the 'scale' function shown in the slides works\n",
    "# a bit differently from what we have encountered in this module so\n",
    "# far; so in the example below, we're using the MinMaxScaler,\n",
    "# which maps our values to [0, 1]. So the functions it uses are a \n",
    "# bit different from the ones shown in the slides\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train, X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did you notice that during the conversion\n",
    "# our data was changed from a Pandas DataFrame \n",
    "# into a simple array? Check with 'type(X_train)'\n",
    "# It's not a big problem,\n",
    "# but we lose the column headers. If we want to\n",
    "# convert them back, we could do:\n",
    "\n",
    "import pandas as pd\n",
    "X_train_pandified = pd.DataFrame(X_train_scaled,columns = X_train.columns)\n",
    "X_train_pandified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120ace6",
   "metadata": {},
   "source": [
    "# Modell berechnen\n",
    "üëè Es folgt nun der Spa√ü: endlich k√∂nnen wir unser erstes Modell trainieren!\n",
    "\n",
    "ü§î Doch: Welches Modell nehmen wir?\n",
    "\n",
    "Da wir eine *Kategorie* vorhersagen m√∂chten (ist eine neue Blume eine *setosa*, eine *versicolor* ...) bietet sich von den Algorithmen, die wir kennengelernt haben, der [Entscheidungsbaum](https://scikit-learn.org/stable/modules/tree.html#tree) an. Tats√§chlich ist die Frage, welcher der [vielen Algorithmen](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) am besten ist, aber eine sehr schwierige, die von der Problemstellung und den Daten abh√§ngt, und viel Erfahrung braucht. Was aber zumindest vereinfacht wird: Die Bedienung aller Algorithmen ist in `sklearn` einheitlich √ºber die `.fit()`-Funktion gestaltet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed1299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT = DT.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c89653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the great advantages of the decision tree is that it is \n",
    "# very easy to understand and see what's happening under the hood\n",
    "# we can even print out the actual tree we just trained!\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "tree.plot_tree(DT, filled=True)\n",
    "plt.show()\n",
    "\n",
    "# as you can see, at every node it asks whether the value\n",
    "# at a specific column is less than some other value to\n",
    "# find a good split for our test data.\n",
    "# By the way, the colors signify as what the data will be\n",
    "# classified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's save our hard work!\n",
    "import joblib\n",
    "joblib.dump(DT, 'iris_tree.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally, if we were out in the field and saw a new\n",
    "# flower, we could now ask our model, what it would think\n",
    "# what type of iris it was:\n",
    "\n",
    "# the only problem is, the data that we trained our model on\n",
    "# was scaled, so we have to also scale our flower the\n",
    "# same way how we scaled the training data\n",
    "\n",
    "new_flower = [[5.1, 3.5, 12, 12]]\n",
    "new_flower_scaled = scaler.transform(pd.DataFrame(new_flower, columns=X_train.columns))\n",
    "\n",
    "# remember that 0, 1 and 2 are the classes - you can see\n",
    "# in the tree above that a very small value in the forth column (X[3])\n",
    "# always means the flower will be of type 0 (setosa), so play around \n",
    "# with the values a bit\n",
    "\n",
    "DT.predict(new_flower_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now since we're not biologists, we can only guess if our model\n",
    "# makes accurate predictions. But, since we have our test-set, we\n",
    "# don't have to guess - we can put our model to the test:\n",
    "\n",
    "# Normally, we'd do this step after computing the model, but in this \n",
    "# demonstation, we're doing it at the end to keep the suspense up! üò¨\n",
    "\n",
    "for X, y in zip(X_test_scaled, y_test):\n",
    "    # zip is a really nifty little helper\n",
    "    # that allows us to go through two lists\n",
    "    # simultaneously\n",
    "    print(DT.predict([X]), y)\n",
    "    \n",
    "# Looking pretty good! üòÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d752139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
